value-based methods conists of critic-only and actor-critic metods.

can be subdivded into:

* [[Monte Carlo Methods]]
* [[Temporal Difference Methods]]

Value based methods can be broken down into (1) [[prediction]] (learning state-value functions (v-functions)) and (2) [[Control Methods]] (learning a policy, often through learning state-action values (q-values)).


can also be subdivided into [[off-policy-methods]] and [[on-policy-methods]]. Note that typical on-policy methods (e.g. SARSA) can easily be extented to be off-policy, but that this would be considered an extension of those algorithms.



![[critic-methods.png]]
For every method:

* Advantages
* Disadvantages
* Use cases
* non-use cases (limitations)


[[Goal of Reinforcement Learning]]

Similar to Dynamic programming, RL assumes some underlying [[Markov decision process (MDP)]] responsible for observed data. But in contrast to DP, RL typically does not assume the MDP dynamics are known (i.e. that no [[full model,  distribution model or full access simulator]] is available). Insead, only the *data* generated by the MDP is available. 


# Use-cases
* When the MDP dynamics are unknown. I.e. when a distribution model is not available.

# Alternatives
*  [[Dynamic Programming]].